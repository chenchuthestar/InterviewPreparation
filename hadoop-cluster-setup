VPC Ranges (Take any one) (IPv4 CIDR block)
-------------------------------------------
192.168.0.0/16
10.0.0.0/16
172.31.0.0/16

Subnet ranges:
-------------
public: 192.168.1.0/24 ,10.0.1.0/24  ,172.31.1.0/24  
private: 192.168.2.0/24 ,10.0.2.0/24 ,172.31.2.0/24

  sudo apt-get update
 sudo apt install openjdk-8-jre-headless
 sudo apt install openjdk-8-jdk-headless
 /usr/lib/jvm/java-8-openjdk-amd64
 
 sudo apt-get install ssh
ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod 0600 ~/.ssh/authorized_keys


 wget http://apachemirror.wuchna.com/hadoop/common/hadoop-2.9.2/hadoop-2.9.2.tar.gz
 
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
export PATH=$PATH:$JAVA_HOME/bin

export HADOOP_HOME=/home/ubuntu/hadoop/hadoop-2.9.2
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin

master -  ip-192-168-1-90.ec2.internal   192.168.1.90
slave - ip-192-168-1-165.ec2.internal     192.168.1.165

hdfs-site.xml
--------------
#dfs.replication  ->  replication configurtion
#dfs.namenode.name.dir -> name node meta data store location 
#dfs.datanode.data.dir -> actual data store location in datanode
<configuration>
  <property>
       <name>dfs.replication</name>
       <value>2</value>
  </property>
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/ubuntu/tmp/hadoop/namenode</value>
  </property>
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/ubuntu/tmp/hadoop/datanode</value>
  </property>
</configuration>



core-site.xml:
-------------

#fs.default.name  ->  name node location and rpc port number
#hadoop.tmp.dir  -> name node temparary local location

<configuration>
    <property>
        <name>fs.default.name</name>
        <value>hdfs://ip-192-168-1-90.ec2.internal:9000</value>
    </property>
    <property>
      <name>hadoop.tmp.dir</name>
      <value>/home/ubuntu/tmp/hadoop/tmp</value>
    </property>
</configuration>



yarn-site.xml
------------
# Configure Resource manager informaton
#yarn.resourcemanager.hostname -> location  where RM will run
#yarn.nodemanager.aux-services,yarn.nodemanager.auxservices.mapreduce.shuffle.class  -> Application master configuration (not a mandatory)

<configuration>
  <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce_shuffle</value>
  </property>
  <property>
      <name>yarn.nodemanager.auxservices.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
  </property>
  <property>
      <name>yarn.resourcemanager.hostname</name>
      <value>ip-192-168-1-165.ec2.internal</value>
      <description>The hostname of the Ressource Manager.</description>
  </property>
</configuration>

netstat -ntlp 



mapred-site.xml
---------------
#To configure Mapreduce framework name information
#mapreduce.framework.name -> provide mapreduce framework name (in mr2 value is yarn)

<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

slave:
------
# to Configure datanodes informations
# ip address of datanodes 
#one ip per line

hadoop-env.sh & mapred-env.sh & yarn-env.sh
--------------------------------------------
export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64 



start-all.sh
mr-jobhistory-daemon.sh start historyserver
stop-all.sh




Task4:
------
Create VPC 
Create private subnet and public subnet
create private and public route tables
Create internet gateway  -> attach VPC
** - attach public route table with internet gate way & aslo atttach public subnet with internet gate way  
	Create elastic ip 
	Create nat gateway (select public subnet)  asign above elatic ip to nat gateway
	attach private route table with nat gate way and also atttach private subnet with nat way  
** -  








{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                        "s3:GetBucketLocation",
                        "s3:ListAllMyBuckets"
                      ],
            "Resource": "arn:aws:s3:::*"
        },
        {
            "Effect": "Allow",
            "Action": "s3:*",
            "Resource": [
                "arn:aws:s3:::task3-user2",
                "arn:aws:s3:::task3-user2/*"
            ]
        }
    ]
}




Task 3:
------
User name,Password,Access key ID,Secret access key,Console login link
task3_user1,,AKIAWGQO2IHG5F3HBWUR,13jUksgYueSpVjHxzmyDwo5pLb3YQYQwRfys/QIC,https://426306585037.signin.aws.amazon.com/console

User name,Password,Access key ID,Secret access key,Console login link
task3_user2,,AKIAWGQO2IHGXBQVDCMJ,tYtLwjGyeoDm6crRnQSIv/+js9K09loxV8woi66Y,https://426306585037.signin.aws.amazon.com/console


Task 2:
-----
aws s3 ls 
 aws s3 cp mumbai_chenchu.pem s3://chenchu-s3-test
 
 Role:
 acces aws services from other aws services;
 Ex: ec2 to s3
     lamda acess s3
	 
user:
	any one new person jons organisation ,he  want aws acesss then create User and assign him to group/Policy.


if any aws service need to aceess another aws service then we have two options
  1 . we can run aws configre cmd (Acess key and secret key) (Not secure)
Role:
----
  2. we will assign a aws role to service to acess another service. (best way& secure)  
  
  
 Difference between IAM role & Acess keys:
  IAM roles we can access one service from another service .it is secure
  Access keys comes wit user creation . by using this we can access any service . these are not secure due to credentials arw save in .aws folder.
	
